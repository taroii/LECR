{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Learning Equality - Curriculum Recommendations Project\n#### By Taro Iyadomi (UCLA Data Theory '24)\n#### 12/23/2022 - Present","metadata":{}},{"cell_type":"code","source":"#### Import necessary packages/functions\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential, Model\nfrom nltk.corpus import stopwords\nimport string\nfrom unidecode import unidecode\nimport random\nimport itertools\nimport csv","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-13T08:47:56.494369Z","iopub.execute_input":"2023-01-13T08:47:56.494811Z","iopub.status.idle":"2023-01-13T08:48:03.593449Z","shell.execute_reply.started":"2023-01-13T08:47:56.494724Z","shell.execute_reply":"2023-01-13T08:48:03.592335Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#### Reading in the data.\ncontent = pd.read_csv(\"/kaggle/input/learning-equality-curriculum-recommendations/content.csv\")\ncorrelations = pd.read_csv(\"/kaggle/input/learning-equality-curriculum-recommendations/correlations.csv\")\n#sample_submission = pd.read_csv(\"/kaggle/input/learning-equality-curriculum-recommendations/sample_submission.csv\")\ntopics = pd.read_csv(\"/kaggle/input/learning-equality-curriculum-recommendations/topics.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-01-13T08:48:03.595411Z","iopub.execute_input":"2023-01-13T08:48:03.596132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### I. Preparing Data","metadata":{}},{"cell_type":"code","source":"#### Create combine function\ndef combine(correlations, topics, content):\n    '''\n    - Inputs our three datasets and combines the topic/content information with the topic/content correlations data. \n    - All topic/content information is concatenated to one \"features\" column, which includes the language, title, description, etc.\n    - Output includes the correlations topics information, correlations content information, and a dictionary to convert indices to their\n      corresponding topic/content id. \n    '''\n    #Drop/combine columns\n    content[\"text\"] = content[\"text\"].fillna('')\n    content = content.dropna()\n    content_combined = content[\"language\"] + \" \" + content[\"title\"] + \" \" + content[\"description\"] + \" \" + content[\"text\"]\n    content_combined = pd.DataFrame({\"id\":content[\"id\"], \"features\":content_combined})\n    print(\"content_combined\", content_combined.shape)\n\n    topics[\"description\"] = topics[\"description\"].fillna('')\n    topics = topics.dropna()\n    topics_combined = topics[\"language\"] + \" \" + topics[\"channel\"] + ' ' + topics[\"title\"] + \" \" + topics[\"description\"]\n    topics_combined = pd.DataFrame({\"id\":topics[\"id\"], \"features\":topics_combined})\n    print(\"topics_combined\", topics_combined.shape)\n    \n    #Explode correlations rows\n    correlations[\"content_ids\"] = correlations[\"content_ids\"].str.split()\n    correlations = correlations.explode(\"content_ids\")\n\n    #Merge\n    merged = correlations.merge(topics_combined, how=\"inner\", left_on=\"topic_id\", right_on=\"id\")\n    print(\"merged\", merged.shape)\n    merged = merged.reset_index().merge(content_combined, how=\"inner\", left_on=\"content_ids\", right_on=\"id\", sort=False, suffixes=(\"_topics\", \"_content\")).sort_values(axis=0, by=\"index\")\n    merged = merged.drop([\"content_ids\", \"topic_id\"], axis=1)\n    print(\"merged\", merged.shape)\n\n    #Split\n    corr_topics = merged[['index', 'features_topics']]\n    corr_topics.columns = ['id', 'features']\n    corr_content = merged[['index', 'features_content']]\n    corr_content.columns = ['id', 'features']\n\n    index_to_topic = pd.Series(merged.id_topics.values, index=merged.index).to_dict()\n    index_to_content = pd.Series(merged.id_content.values, index=merged.index).to_dict()\n\n    return corr_topics, corr_content, index_to_topic, index_to_content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Apply combine() to our data\ncorr_topics, corr_content, index_to_topic, index_to_content = combine(correlations, topics, content)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Create a stopword removal function to remove stopwords for each language\n\n# Dictionary of languages found in our data\nlang_dict = {\n    \"en\":\"english\",\n    \"es\":\"spanish\",\n    \"it\":\"italian\",\n    'pt':\"portuguese\",\n    'mr':'marathi',\n    'bg':'bulgarian',\n    'gu':'gujarati',\n    'sw':'swahili',\n    'hi':'hindi',\n    'ar':'arabic',\n    'bn':'bengali',\n    'as':'assamese',\n    'zh':'chinese',\n    'fr':'french',\n    'km':'khmer',\n    'pl':'polish',\n    'ta':'tamil',\n    'or':'oriya',\n    'ru':'russian',\n    'kn':'kannada',\n    'swa':'swahili',\n    'my':'burmese',\n    'pnb':'punjabi',\n    'fil':'filipino',\n    'tr':'turkish',\n    'te':'telugu',\n    'ur':'urdu',\n    'fi':'finnish',\n    'pn':'unknown'}\n\n# List of languages supported by the natural language tool kit (NLTK) module.\nsupported_languages = stopwords.fileids()\n\ndef remove_stopwords(text):\n    '''\n    Checks language of text then removes stopwords from that language if supported.\n    '''\n    lang_code = text[0:2]\n    if lang_dict[lang_code] in supported_languages:\n        for word in stopwords.words(lang_dict[lang_code]):\n            text = text.replace(' ' + word + ' ', ' ')\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Apply remove_stopwords() to our data\ncorr_topics[\"features\"] = corr_topics.features.apply(remove_stopwords)\ncorr_content[\"features\"] = corr_content.features.apply(remove_stopwords)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Create train/test indices for our data \nrandom.seed(10)\ntrain_indices = random.sample(range(len(corr_content)), round(0.8*len(corr_content))) #80/20 train/test split\n\n#### Split training data so 50% is matching and 50% is not matching\nhalf = round(len(train_indices) / 2)\nfull = len(train_indices)\n\ntrain_topics_half = corr_topics.iloc[train_indices[:half], :]\ntrain_content_half = corr_content.iloc[train_indices[:half], :]\n\n#Shift second half so that topics/content are not matching\ntrain_topics_full = corr_topics.iloc[train_indices[half:(full-20)], :] \ntrain_content_full = corr_content.iloc[train_indices[(half+20):(full)], :] \n\ntrain_topics = pd.concat([train_topics_half, train_topics_full]).reset_index().drop(\"index\", axis=1)\ntrain_content = pd.concat([train_content_half, train_content_full]).reset_index().drop(\"index\", axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Repeat for test data\ntest_topics = corr_topics.drop(train_indices, axis=0)\ntest_content = corr_content.drop(train_indices, axis=0)\n\nhalf = round(len(test_topics.features) / 2)\nfull = len(test_topics.features)\n\ntest_topics_half = test_topics.iloc[:half, :]\ntest_content_half = test_content.iloc[:half, :]\n\ntest_topics_full = test_topics.iloc[half:(full - 5), :]\ntest_content_full = test_content.iloc[(half+5):full, :]\n\ntest_topics = pd.concat([test_topics_half, test_topics_full]).reset_index().drop(\"index\", axis=1)\ntest_content = pd.concat([test_content_half, test_content_full]).reset_index().drop(\"index\", axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Create labels\ntrain_labels = np.array((train_topics.id == train_content.id).astype(int))\ntest_labels = np.array((test_topics.id == test_content.id).astype(int))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Convert data to tensors\ntrain_topics = tf.data.Dataset.from_tensor_slices(tf.cast(train_topics.features, tf.string))\ntrain_content = tf.data.Dataset.from_tensor_slices(tf.cast(train_content.features, tf.string))\ntrain_labels = tf.data.Dataset.from_tensor_slices(tf.cast(train_labels, tf.int32))\n\ntest_topics = tf.data.Dataset.from_tensor_slices(tf.cast(test_topics.features, tf.string))\ntest_content = tf.data.Dataset.from_tensor_slices(tf.cast(test_content.features, tf.string))\ntest_labels = tf.data.Dataset.from_tensor_slices(tf.cast(test_labels, tf.int32))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Combine data into TensorFlow Datasets\n#### Perfectly shuffle, batch, cache, and prefetch our new datasets\ntrain_ds = tf.data.Dataset.zip(\n    ((train_topics, train_content), train_labels)\n)\n\ntrain_ds = train_ds.shuffle(buffer_size = train_ds.cardinality().numpy()).batch(batch_size = 64).cache().prefetch(tf.data.experimental.AUTOTUNE)\n\ntest_ds = tf.data.Dataset.zip(\n    ((test_topics, test_content), test_labels)\n)\n\ntest_ds = test_ds.shuffle(buffer_size = test_ds.cardinality().numpy()).batch(batch_size = 64).cache().prefetch(tf.data.experimental.AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## II. Building the Model","metadata":{}},{"cell_type":"code","source":"#### Create Text Vectorization Layer\n# Hyperparameters\nVOCAB_SIZE = 1000000\nMAX_LEN = 50\n\ndef my_standardize(text): \n    '''\n    A text standardization function that is applied for every element in the vectorize layer. \n    '''\n    text = tf.strings.lower(text, encoding='utf-8') #lowercase\n    text = tf.strings.regex_replace(text, f\"([{string.punctuation}])\", r\" \") #remove punctuation\n    text = tf.strings.regex_replace(text, '\\n', \"\") #remove newlines\n    text = tf.strings.regex_replace(text, ' +', \" \") #remove 2+ whitespaces\n    text = tf.strings.strip(text) #remove leading and tailing whitespaces\n    return text\n\nvectorize_layer = TextVectorization(\n    standardize = my_standardize,\n    split = \"whitespace\",\n    max_tokens = VOCAB_SIZE + 2,\n    output_mode = 'int',\n    output_sequence_length = MAX_LEN\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Adapt text vectorization layer to our data\nvectorize_layer.adapt(pd.concat([corr_topics[\"features\"], corr_content[\"features\"]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp_topics = Input((1, ), dtype=tf.string)\ninp_content = Input((1, ), dtype=tf.string)\n\nvectorized_topics = vectorize_layer(inp_topics)\nvectorized_content = vectorize_layer(inp_content)\n\nsnn = Sequential([ \n  Embedding(VOCAB_SIZE, 256),\n  GlobalAveragePooling1D(),\n  Flatten(),\n  Dense(128, activation='relu'),\n])\n\nsnn_content = snn(vectorized_content)\nsnn_topics = snn(vectorized_topics)\n\nconcat = Concatenate()([snn_topics, snn_content])\n\ndense = Dense(64, activation='relu')(concat)\n\noutput = Dense(1, activation='sigmoid')(dense)\n\nmodel = Model(inputs=[inp_topics, inp_content], outputs=output)\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=tf.keras.metrics.AUC())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_ds, epochs=5, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## III. Evaluating the Model","metadata":{}},{"cell_type":"code","source":"model.evaluate(test_ds, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## IV. Making Predictions","metadata":{}},{"cell_type":"markdown","source":"We must now make predictions on the topics dataset. While the topics dataset is very large, the correlations data makes up for a large portion of it. So, we are left with only about 15,000 topics to cover!\n","metadata":{}},{"cell_type":"code","source":"#### Antijoin topics with correlations data, since we don't have to predict those topics\nouter_joined = topics.merge(correlations, how='outer', left_on='id', right_on='topic_id', indicator=True)\ntopics = outer_joined[(outer_joined._merge == 'left_only')].drop('_merge', axis=1)\n\n#### Fill missing values and concatenate text to features column \ntopics = topics.fillna(\"\")\ntopics_ids = topics.id.values\ntopics_lang = topics.language\ntopics_index = topics.index\ntopics_features = topics[\"language\"] + ' ' + topics[\"channel\"] + ' ' + topics[\"title\"] + ' ' + topics[\"description\"]\ndel topics\n\n#### Repeat for content, except we keep all content data\ncontent = content.fillna(\"\")\ncontent_ids = content.id.values\ncontent_index = content.index\ncontent_lang = content.language\ncontent_features = content[\"language\"] + ' ' + content[\"title\"] + ' ' + content[\"description\"] + ' ' + content[\"text\"]\ndel content\n\nindex_to_content = pd.Series(content_ids, index=content_index).to_dict()\nindex_to_topic = pd.Series(topics_ids, index=topics_index).to_dict()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Remove stopwords\ntopics_features = topics_features.apply(remove_stopwords)\ncontent_features = content_features.apply(remove_stopwords)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Write predictions to output_file\nTHRESHOLD = 0.99994\n\noutput_file = \"submission.csv\"\nf = open(output_file, 'w')\n\nwriter = csv.writer(f)\nwriter.writerow([\"topic_id\", \"content_ids\"])\n\nfor i in topics_features.index:\n    temp_content = tf.data.Dataset.from_tensor_slices(\n        tf.cast(content_features[content_lang == topics_lang[i]], tf.string)\n    )\n    temp_topic = tf.data.Dataset.from_tensor_slices(\n        tf.cast(np.repeat(topics_features[i], len(temp_content)), tf.string)\n    )\n    temp_ds = tf.data.Dataset.zip(((temp_topic, temp_content), ))\\\n        .batch(batch_size=64)\\\n            .cache()\\\n                .prefetch(tf.data.experimental.AUTOTUNE)\n    matches = model.predict(temp_ds, verbose=0)\n    matches = [i for i in range(len(matches)) if matches[i] > THRESHOLD]\n    matches = \" \".join([index_to_content[x] for x in matches])\n    writer.writerow([index_to_topic[i], matches])\n\nf.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}