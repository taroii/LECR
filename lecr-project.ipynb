{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Learning Equality - Curriculum Recommendations Project\n#### By Taro Iyadomi (UCLA Data Theory '24)\n#### 12/23/2022 - Present","metadata":{}},{"cell_type":"code","source":"#### Import necessary packages/functions\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential, Model\nfrom nltk.corpus import stopwords\nimport string\nfrom unidecode import unidecode\nimport random\nimport itertools\nimport csv","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-13T08:47:56.494369Z","iopub.execute_input":"2023-01-13T08:47:56.494811Z","iopub.status.idle":"2023-01-13T08:48:03.593449Z","shell.execute_reply.started":"2023-01-13T08:47:56.494724Z","shell.execute_reply":"2023-01-13T08:48:03.592335Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#### Reading in the data.\ncontent = pd.read_csv(\"/kaggle/input/learning-equality-curriculum-recommendations/content.csv\")\ncorrelations = pd.read_csv(\"/kaggle/input/learning-equality-curriculum-recommendations/correlations.csv\")\n#sample_submission = pd.read_csv(\"/kaggle/input/learning-equality-curriculum-recommendations/sample_submission.csv\")\ntopics = pd.read_csv(\"/kaggle/input/learning-equality-curriculum-recommendations/topics.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-01-13T08:48:03.595411Z","iopub.execute_input":"2023-01-13T08:48:03.596132Z","iopub.status.idle":"2023-01-13T08:48:25.141912Z","shell.execute_reply.started":"2023-01-13T08:48:03.596096Z","shell.execute_reply":"2023-01-13T08:48:25.140884Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### I. Preparing Data","metadata":{}},{"cell_type":"code","source":"#### Create combine function\ndef combine(correlations, topics, content):\n    '''\n    - Inputs our three datasets and combines the topic/content information with the topic/content correlations data. \n    - All topic/content information is concatenated to one \"features\" column, which includes the language, title, description, etc.\n    - Output includes the correlations topics information, correlations content information, and a dictionary to convert indices to their\n      corresponding topic/content id. \n    '''\n    #Drop/combine columns\n    content[\"text\"] = content[\"text\"].fillna('')\n    content = content.dropna()\n    content_combined = content[\"language\"] + \" \" + content[\"title\"] + \" \" + content[\"description\"] + \" \" + content[\"text\"]\n    content_combined = pd.DataFrame({\"id\":content[\"id\"], \"features\":content_combined})\n    print(\"content_combined\", content_combined.shape)\n\n    topics[\"description\"] = topics[\"description\"].fillna('')\n    topics = topics.dropna()\n    topics_combined = topics[\"language\"] + \" \" + topics[\"channel\"] + ' ' + topics[\"title\"] + \" \" + topics[\"description\"]\n    topics_combined = pd.DataFrame({\"id\":topics[\"id\"], \"features\":topics_combined})\n    print(\"topics_combined\", topics_combined.shape)\n    \n    #Explode correlations rows\n    correlations[\"content_ids\"] = correlations[\"content_ids\"].str.split()\n    correlations = correlations.explode(\"content_ids\")\n\n    #Merge\n    merged = correlations.merge(topics_combined, how=\"inner\", left_on=\"topic_id\", right_on=\"id\")\n    print(\"merged\", merged.shape)\n    merged = merged.reset_index().merge(content_combined, how=\"inner\", left_on=\"content_ids\", right_on=\"id\", sort=False, suffixes=(\"_topics\", \"_content\")).sort_values(axis=0, by=\"index\")\n    merged = merged.drop([\"content_ids\", \"topic_id\"], axis=1)\n    print(\"merged\", merged.shape)\n\n    #Split\n    corr_topics = merged[['index', 'features_topics']]\n    corr_topics.columns = ['id', 'features']\n    corr_content = merged[['index', 'features_content']]\n    corr_content.columns = ['id', 'features']\n\n    index_to_topic = pd.Series(merged.id_topics.values, index=merged.index).to_dict()\n    index_to_content = pd.Series(merged.id_content.values, index=merged.index).to_dict()\n\n    return corr_topics, corr_content, index_to_topic, index_to_content","metadata":{"execution":{"iopub.status.busy":"2023-01-13T08:48:25.143744Z","iopub.execute_input":"2023-01-13T08:48:25.144174Z","iopub.status.idle":"2023-01-13T08:48:25.158085Z","shell.execute_reply.started":"2023-01-13T08:48:25.144133Z","shell.execute_reply":"2023-01-13T08:48:25.155340Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#### Apply combine() to our data\ncorr_topics, corr_content, index_to_topic, index_to_content = combine(correlations, topics, content)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T08:48:25.161095Z","iopub.execute_input":"2023-01-13T08:48:25.161507Z","iopub.status.idle":"2023-01-13T08:48:26.504043Z","shell.execute_reply.started":"2023-01-13T08:48:25.161453Z","shell.execute_reply":"2023-01-13T08:48:26.502169Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"content_combined (41621, 2)\ntopics_combined (76799, 2)\nmerged (279892, 4)\nmerged (84238, 5)\n","output_type":"stream"}]},{"cell_type":"code","source":"#### Create a stopword removal function to remove stopwords for each language\n\n# Dictionary of languages found in our data\nlang_dict = {\n    \"en\":\"english\",\n    \"es\":\"spanish\",\n    \"it\":\"italian\",\n    'pt':\"portuguese\",\n    'mr':'marathi',\n    'bg':'bulgarian',\n    'gu':'gujarati',\n    'sw':'swahili',\n    'hi':'hindi',\n    'ar':'arabic',\n    'bn':'bengali',\n    'as':'assamese',\n    'zh':'chinese',\n    'fr':'french',\n    'km':'khmer',\n    'pl':'polish',\n    'ta':'tamil',\n    'or':'oriya',\n    'ru':'russian',\n    'kn':'kannada',\n    'swa':'swahili',\n    'my':'burmese',\n    'pnb':'punjabi',\n    'fil':'filipino',\n    'tr':'turkish',\n    'te':'telugu',\n    'ur':'urdu',\n    'fi':'finnish',\n    'pn':'unknown',\n    'mu':'unknown'}\n\n# List of languages supported by the natural language tool kit (NLTK) module.\nsupported_languages = stopwords.fileids()\n\ndef remove_stopwords(text):\n    '''\n    Checks language of text then removes stopwords from that language if supported.\n    '''\n    lang_code = text[0:2]\n    if lang_dict[lang_code] in supported_languages:\n        for word in stopwords.words(lang_dict[lang_code]):\n            text = text.replace(' ' + word + ' ', ' ')\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-01-13T09:14:43.345414Z","iopub.execute_input":"2023-01-13T09:14:43.345776Z","iopub.status.idle":"2023-01-13T09:14:43.354976Z","shell.execute_reply.started":"2023-01-13T09:14:43.345745Z","shell.execute_reply":"2023-01-13T09:14:43.353859Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"#### Apply remove_stopwords() to our data\ncorr_topics[\"features\"] = corr_topics.features.apply(remove_stopwords)\ncorr_content[\"features\"] = corr_content.features.apply(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T08:48:26.525833Z","iopub.execute_input":"2023-01-13T08:48:26.526164Z","iopub.status.idle":"2023-01-13T08:51:58.432973Z","shell.execute_reply.started":"2023-01-13T08:48:26.526139Z","shell.execute_reply":"2023-01-13T08:51:58.431817Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#### Create train/test indices for our data \nrandom.seed(10)\ntrain_indices = random.sample(range(len(corr_content)), round(0.8*len(corr_content))) #80/20 train/test split\n\n#### Split training data so 50% is matching and 50% is not matching\nhalf = round(len(train_indices) / 2)\nfull = len(train_indices)\n\ntrain_topics_half = corr_topics.iloc[train_indices[:half], :]\ntrain_content_half = corr_content.iloc[train_indices[:half], :]\n\n#Shift second half so that topics/content are not matching\ntrain_topics_full = corr_topics.iloc[train_indices[half:(full-20)], :] \ntrain_content_full = corr_content.iloc[train_indices[(half+20):(full)], :] \n\ntrain_topics = pd.concat([train_topics_half, train_topics_full]).reset_index().drop(\"index\", axis=1)\ntrain_content = pd.concat([train_content_half, train_content_full]).reset_index().drop(\"index\", axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T08:51:58.443408Z","iopub.execute_input":"2023-01-13T08:51:58.444131Z","iopub.status.idle":"2023-01-13T08:51:58.635967Z","shell.execute_reply.started":"2023-01-13T08:51:58.444094Z","shell.execute_reply":"2023-01-13T08:51:58.634803Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#### Repeat for test data\ntest_topics = corr_topics.drop(train_indices, axis=0)\ntest_content = corr_content.drop(train_indices, axis=0)\n\nhalf = round(len(test_topics.features) / 2)\nfull = len(test_topics.features)\n\ntest_topics_half = test_topics.iloc[:half, :]\ntest_content_half = test_content.iloc[:half, :]\n\ntest_topics_full = test_topics.iloc[half:(full - 5), :]\ntest_content_full = test_content.iloc[(half+5):full, :]\n\ntest_topics = pd.concat([test_topics_half, test_topics_full]).reset_index().drop(\"index\", axis=1)\ntest_content = pd.concat([test_content_half, test_content_full]).reset_index().drop(\"index\", axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T08:51:58.637787Z","iopub.execute_input":"2023-01-13T08:51:58.638574Z","iopub.status.idle":"2023-01-13T08:51:58.696409Z","shell.execute_reply.started":"2023-01-13T08:51:58.638534Z","shell.execute_reply":"2023-01-13T08:51:58.695404Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#### Create labels\ntrain_labels = np.array((train_topics.id == train_content.id).astype(int))\ntest_labels = np.array((test_topics.id == test_content.id).astype(int))","metadata":{"execution":{"iopub.status.busy":"2023-01-13T08:51:58.698175Z","iopub.execute_input":"2023-01-13T08:51:58.698893Z","iopub.status.idle":"2023-01-13T08:51:58.706994Z","shell.execute_reply.started":"2023-01-13T08:51:58.698838Z","shell.execute_reply":"2023-01-13T08:51:58.705893Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#### Convert data to tensors\ntrain_topics = tf.data.Dataset.from_tensor_slices(tf.cast(train_topics.features, tf.string))\ntrain_content = tf.data.Dataset.from_tensor_slices(tf.cast(train_content.features, tf.string))\ntrain_labels = tf.data.Dataset.from_tensor_slices(tf.cast(train_labels, tf.int32))\n\ntest_topics = tf.data.Dataset.from_tensor_slices(tf.cast(test_topics.features, tf.string))\ntest_content = tf.data.Dataset.from_tensor_slices(tf.cast(test_content.features, tf.string))\ntest_labels = tf.data.Dataset.from_tensor_slices(tf.cast(test_labels, tf.int32))","metadata":{"execution":{"iopub.status.busy":"2023-01-13T08:51:58.712237Z","iopub.execute_input":"2023-01-13T08:51:58.712997Z","iopub.status.idle":"2023-01-13T08:52:03.871425Z","shell.execute_reply.started":"2023-01-13T08:51:58.712962Z","shell.execute_reply":"2023-01-13T08:52:03.870362Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"2023-01-13 08:51:58.845365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-13 08:51:58.846598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-13 08:51:59.120719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-13 08:51:59.121989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-13 08:51:59.123084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-13 08:51:59.124140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-13 08:51:59.128497: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-01-13 08:51:59.423830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-13 08:51:59.425080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-13 08:51:59.426190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-13 08:51:59.427257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-13 08:51:59.428327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-13 08:51:59.429364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-13 08:52:02.567399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-13 08:52:02.568820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-13 08:52:02.570007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-13 08:52:02.571150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-13 08:52:02.572258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-13 08:52:02.573266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13789 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n2023-01-13 08:52:02.577831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-13 08:52:02.578829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13789 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}]},{"cell_type":"code","source":"#### Combine data into TensorFlow Datasets\n#### Perfectly shuffle, batch, cache, and prefetch our new datasets\ntrain_ds = tf.data.Dataset.zip(\n    ((train_topics, train_content), train_labels)\n)\n\ntrain_ds = train_ds.shuffle(buffer_size = train_ds.cardinality().numpy()).batch(batch_size = 64).cache().prefetch(tf.data.experimental.AUTOTUNE)\n\ntest_ds = tf.data.Dataset.zip(\n    ((test_topics, test_content), test_labels)\n)\n\ntest_ds = test_ds.shuffle(buffer_size = test_ds.cardinality().numpy()).batch(batch_size = 64).cache().prefetch(tf.data.experimental.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T08:52:03.873013Z","iopub.execute_input":"2023-01-13T08:52:03.873458Z","iopub.status.idle":"2023-01-13T08:52:03.895500Z","shell.execute_reply.started":"2023-01-13T08:52:03.873420Z","shell.execute_reply":"2023-01-13T08:52:03.894577Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## II. Building the Model","metadata":{}},{"cell_type":"code","source":"#### Create Text Vectorization Layer\n# Hyperparameters\nVOCAB_SIZE = 1000000\nMAX_LEN = 50\n\ndef my_standardize(text): \n    '''\n    A text standardization function that is applied for every element in the vectorize layer. \n    '''\n    text = tf.strings.lower(text, encoding='utf-8') #lowercase\n    text = tf.strings.regex_replace(text, f\"([{string.punctuation}])\", r\" \") #remove punctuation\n    text = tf.strings.regex_replace(text, '\\n', \"\") #remove newlines\n    text = tf.strings.regex_replace(text, ' +', \" \") #remove 2+ whitespaces\n    text = tf.strings.strip(text) #remove leading and tailing whitespaces\n    return text\n\nvectorize_layer = TextVectorization(\n    standardize = my_standardize,\n    split = \"whitespace\",\n    max_tokens = VOCAB_SIZE + 2,\n    output_mode = 'int',\n    output_sequence_length = MAX_LEN\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T08:52:03.896747Z","iopub.execute_input":"2023-01-13T08:52:03.897212Z","iopub.status.idle":"2023-01-13T08:52:03.929003Z","shell.execute_reply.started":"2023-01-13T08:52:03.897177Z","shell.execute_reply":"2023-01-13T08:52:03.928117Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#### Adapt text vectorization layer to our data\nvectorize_layer.adapt(pd.concat([corr_topics[\"features\"], corr_content[\"features\"]]))","metadata":{"execution":{"iopub.status.busy":"2023-01-13T08:52:03.930394Z","iopub.execute_input":"2023-01-13T08:52:03.930737Z","iopub.status.idle":"2023-01-13T08:52:58.026503Z","shell.execute_reply.started":"2023-01-13T08:52:03.930704Z","shell.execute_reply":"2023-01-13T08:52:58.025376Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"2023-01-13 08:52:04.559865: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"inp_topics = Input((1, ), dtype=tf.string)\ninp_content = Input((1, ), dtype=tf.string)\n\nvectorized_topics = vectorize_layer(inp_topics)\nvectorized_content = vectorize_layer(inp_content)\n\nsnn = Sequential([ \n  Embedding(VOCAB_SIZE, 256),\n  GlobalAveragePooling1D(),\n  Flatten(),\n  Dense(128, activation='relu'),\n])\n\nsnn_content = snn(vectorized_content)\nsnn_topics = snn(vectorized_topics)\n\nconcat = Concatenate()([snn_topics, snn_content])\n\ndense = Dense(64, activation='relu')(concat)\n\noutput = Dense(1, activation='sigmoid')(dense)\n\nmodel = Model(inputs=[inp_topics, inp_content], outputs=output)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-01-13T08:52:58.027973Z","iopub.execute_input":"2023-01-13T08:52:58.028347Z","iopub.status.idle":"2023-01-13T08:52:58.309934Z","shell.execute_reply.started":"2023-01-13T08:52:58.028309Z","shell.execute_reply":"2023-01-13T08:52:58.308908Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 1)]          0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 1)]          0                                            \n__________________________________________________________________________________________________\ntext_vectorization (TextVectori (None, 50)           0           input_1[0][0]                    \n                                                                 input_2[0][0]                    \n__________________________________________________________________________________________________\nsequential (Sequential)         (None, 128)          256032896   text_vectorization[1][0]         \n                                                                 text_vectorization[0][0]         \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 256)          0           sequential[1][0]                 \n                                                                 sequential[0][0]                 \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 64)           16448       concatenate[0][0]                \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1)            65          dense_1[0][0]                    \n==================================================================================================\nTotal params: 256,049,409\nTrainable params: 256,049,409\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=tf.keras.metrics.AUC())","metadata":{"execution":{"iopub.status.busy":"2023-01-13T08:52:58.311235Z","iopub.execute_input":"2023-01-13T08:52:58.311601Z","iopub.status.idle":"2023-01-13T08:52:58.330830Z","shell.execute_reply.started":"2023-01-13T08:52:58.311563Z","shell.execute_reply":"2023-01-13T08:52:58.329981Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model.fit(train_ds, epochs=5, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T08:52:58.332576Z","iopub.execute_input":"2023-01-13T08:52:58.333010Z","iopub.status.idle":"2023-01-13T09:06:06.522876Z","shell.execute_reply.started":"2023-01-13T08:52:58.332971Z","shell.execute_reply":"2023-01-13T09:06:06.521780Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/5\n1053/1053 [==============================] - 149s 139ms/step - loss: 0.4420 - auc: 0.8571\nEpoch 2/5\n1053/1053 [==============================] - 145s 138ms/step - loss: 0.2822 - auc: 0.9417\nEpoch 3/5\n1053/1053 [==============================] - 146s 138ms/step - loss: 0.1999 - auc: 0.9724\nEpoch 4/5\n1053/1053 [==============================] - 146s 138ms/step - loss: 0.1511 - auc: 0.9846\nEpoch 5/5\n1053/1053 [==============================] - 146s 138ms/step - loss: 0.1229 - auc: 0.9898\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f2203de80d0>"},"metadata":{}}]},{"cell_type":"markdown","source":"## III. Evaluating the Model","metadata":{}},{"cell_type":"code","source":"model.evaluate(test_ds, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T09:06:06.524902Z","iopub.execute_input":"2023-01-13T09:06:06.525664Z","iopub.status.idle":"2023-01-13T09:06:15.908369Z","shell.execute_reply.started":"2023-01-13T09:06:06.525626Z","shell.execute_reply":"2023-01-13T09:06:15.907291Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"264/264 [==============================] - 9s 34ms/step - loss: 0.7072 - auc: 0.8648\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"[0.7071595191955566, 0.8648307919502258]"},"metadata":{}}]},{"cell_type":"markdown","source":"## IV. Making Predictions","metadata":{}},{"cell_type":"markdown","source":"We must now make predictions on the topics dataset. While the topics dataset is very large, the correlations data makes up for a large portion of it. So, we are left with only about 15,000 topics to cover!\n","metadata":{}},{"cell_type":"code","source":"#### Antijoin topics with correlations data, since we don't have to predict those topics\nouter_joined = topics.merge(correlations, how='outer', left_on='id', right_on='topic_id', indicator=True)\ntopics = outer_joined[(outer_joined._merge == 'left_only')].drop('_merge', axis=1)\n\n#### Fill missing values and concatenate text to features column \ntopics = topics.fillna(\"\")\ntopics_ids = topics.id.values\ntopics_lang = topics.language\ntopics_index = topics.index\ntopics_features = topics[\"language\"] + ' ' + topics[\"channel\"] + ' ' + topics[\"title\"] + ' ' + topics[\"description\"]\ndel topics\n\n#### Repeat for content, except we keep all content data\ncontent = content.fillna(\"\")\ncontent_ids = content.id.values\ncontent_index = content.index\ncontent_lang = content.language\ncontent_features = content[\"language\"] + ' ' + content[\"title\"] + ' ' + content[\"description\"] + ' ' + content[\"text\"]\ndel content\n\nindex_to_content = pd.Series(content_ids, index=content_index).to_dict()\nindex_to_topic = pd.Series(topics_ids, index=topics_index).to_dict()","metadata":{"execution":{"iopub.status.busy":"2023-01-13T09:06:15.910948Z","iopub.execute_input":"2023-01-13T09:06:15.911293Z","iopub.status.idle":"2023-01-13T09:06:17.210952Z","shell.execute_reply.started":"2023-01-13T09:06:15.911263Z","shell.execute_reply":"2023-01-13T09:06:17.209897Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#### Remove stopwords\ntopics_features = topics_features.apply(remove_stopwords)\ncontent_features = content_features.apply(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T09:14:58.983919Z","iopub.execute_input":"2023-01-13T09:14:58.984913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Write predictions to output_file\nTHRESHOLD = 0.99994\n\noutput_file = \"submission.csv\"\nf = open(output_file, 'w')\n\nwriter = csv.writer(f)\nwriter.writerow([\"topic_id\", \"content_ids\"])\n\nfor i in topics_features.index:\n    temp_content = tf.data.Dataset.from_tensor_slices(\n        tf.cast(content_features[content_lang == topics_lang[i]], tf.string)\n    )\n    temp_topic = tf.data.Dataset.from_tensor_slices(\n        tf.cast(np.repeat(topics_features[i], len(temp_content)), tf.string)\n    )\n    temp_ds = tf.data.Dataset.zip(((temp_topic, temp_content), ))\\\n        .batch(batch_size=64)\\\n            .cache()\\\n                .prefetch(tf.data.experimental.AUTOTUNE)\n    matches = model.predict(temp_ds, verbose=0)\n    matches = [i for i in range(len(matches)) if matches[i] > THRESHOLD]\n    matches = \" \".join([index_to_content[x] for x in matches])\n    writer.writerow([index_to_topic[i], matches])\n\n#### Add given correlations data\nwriter.writerows([correlations.topic_id, correlations.content_ids])    \n\nf.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}