{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Equality - Curriculum Recommendations Project\n",
    "\n",
    "#### Taro Iyadomi (UCLA Data Theory '24)\n",
    "\n",
    "#### 12/23/2022 - Present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Viewing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>content_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_1108dd0c7a5d c_376c5a8eb028 c_5bc0e1e2cba0 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_00068291e9a4</td>\n",
       "      <td>c_639ea2ef9c95 c_89ce9367be10 c_ac1672cdcd2c c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_11a1dc0bfb99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_0006d41a73a8</td>\n",
       "      <td>c_0c6473c3480d c_1c57a1316568 c_5e375cf14c47 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_4054df11a74e</td>\n",
       "      <td>c_3695c5dc1df6 c_f2d184a98231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic_id                                        content_ids\n",
       "0  t_00004da3a1b2  c_1108dd0c7a5d c_376c5a8eb028 c_5bc0e1e2cba0 c...\n",
       "1  t_00068291e9a4  c_639ea2ef9c95 c_89ce9367be10 c_ac1672cdcd2c c...\n",
       "2  t_00069b63a70a                                     c_11a1dc0bfb99\n",
       "3  t_0006d41a73a8  c_0c6473c3480d c_1c57a1316568 c_5e375cf14c47 c...\n",
       "4  t_4054df11a74e                      c_3695c5dc1df6 c_f2d184a98231"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample Submission\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "print(np.shape(sample_submission))\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61517, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>content_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_1108dd0c7a5d c_376c5a8eb028 c_5bc0e1e2cba0 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_00068291e9a4</td>\n",
       "      <td>c_639ea2ef9c95 c_89ce9367be10 c_ac1672cdcd2c c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_11a1dc0bfb99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_0006d41a73a8</td>\n",
       "      <td>c_0c6473c3480d c_1c57a1316568 c_5e375cf14c47 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_0008768bdee6</td>\n",
       "      <td>c_34e1424229b4 c_7d1a964d66d5 c_aab93ee667f4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic_id                                        content_ids\n",
       "0  t_00004da3a1b2  c_1108dd0c7a5d c_376c5a8eb028 c_5bc0e1e2cba0 c...\n",
       "1  t_00068291e9a4  c_639ea2ef9c95 c_89ce9367be10 c_ac1672cdcd2c c...\n",
       "2  t_00069b63a70a                                     c_11a1dc0bfb99\n",
       "3  t_0006d41a73a8  c_0c6473c3480d c_1c57a1316568 c_5e375cf14c47 c...\n",
       "4  t_0008768bdee6       c_34e1424229b4 c_7d1a964d66d5 c_aab93ee667f4"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Correlations\n",
    "correlations = pd.read_csv(\"correlations.csv\")\n",
    "print(np.shape(correlations))\n",
    "correlations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76972, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>channel</th>\n",
       "      <th>category</th>\n",
       "      <th>level</th>\n",
       "      <th>language</th>\n",
       "      <th>parent</th>\n",
       "      <th>has_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>Откриването на резисторите</td>\n",
       "      <td>Изследване на материали, които предизвикват на...</td>\n",
       "      <td>000cf7</td>\n",
       "      <td>source</td>\n",
       "      <td>4</td>\n",
       "      <td>bg</td>\n",
       "      <td>t_16e29365b50d</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_000095e03056</td>\n",
       "      <td>Unit 3.3 Enlargements and Similarities</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b3f329</td>\n",
       "      <td>aligned</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>t_aa32fb6252dc</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_00068291e9a4</td>\n",
       "      <td>Entradas e saídas de uma função</td>\n",
       "      <td>Entenda um pouco mais sobre funções.</td>\n",
       "      <td>8e286a</td>\n",
       "      <td>source</td>\n",
       "      <td>4</td>\n",
       "      <td>pt</td>\n",
       "      <td>t_d14b6c2a2b70</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>Transcripts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6e3ba4</td>\n",
       "      <td>source</td>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>t_4054df11a74e</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_0006d41a73a8</td>\n",
       "      <td>Графики на експоненциални функции (Алгебра 2 н...</td>\n",
       "      <td>Научи повече за графиките на сложните показате...</td>\n",
       "      <td>000cf7</td>\n",
       "      <td>source</td>\n",
       "      <td>4</td>\n",
       "      <td>bg</td>\n",
       "      <td>t_e2452e21d252</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                              title  \\\n",
       "0  t_00004da3a1b2                         Откриването на резисторите   \n",
       "1  t_000095e03056             Unit 3.3 Enlargements and Similarities   \n",
       "2  t_00068291e9a4                    Entradas e saídas de uma função   \n",
       "3  t_00069b63a70a                                        Transcripts   \n",
       "4  t_0006d41a73a8  Графики на експоненциални функции (Алгебра 2 н...   \n",
       "\n",
       "                                         description channel category  level  \\\n",
       "0  Изследване на материали, които предизвикват на...  000cf7   source      4   \n",
       "1                                                NaN  b3f329  aligned      2   \n",
       "2               Entenda um pouco mais sobre funções.  8e286a   source      4   \n",
       "3                                                NaN  6e3ba4   source      3   \n",
       "4  Научи повече за графиките на сложните показате...  000cf7   source      4   \n",
       "\n",
       "  language          parent  has_content  \n",
       "0       bg  t_16e29365b50d         True  \n",
       "1       en  t_aa32fb6252dc        False  \n",
       "2       pt  t_d14b6c2a2b70         True  \n",
       "3       en  t_4054df11a74e         True  \n",
       "4       bg  t_e2452e21d252         True  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Topics\n",
    "topics = pd.read_csv(\"topics.csv\")\n",
    "print(np.shape(topics))\n",
    "topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154047, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>kind</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>copyright_holder</th>\n",
       "      <th>license</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c_00002381196d</td>\n",
       "      <td>Sumar números de varios dígitos: 48,029+233,930</td>\n",
       "      <td>Suma 48,029+233,930 mediante el algoritmo está...</td>\n",
       "      <td>video</td>\n",
       "      <td>NaN</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c_000087304a9e</td>\n",
       "      <td>Trovare i fattori di un numero</td>\n",
       "      <td>Sal trova i fattori di 120.\\n\\n</td>\n",
       "      <td>video</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c_0000ad142ddb</td>\n",
       "      <td>Sumar curvas de demanda</td>\n",
       "      <td>Cómo añadir curvas de demanda\\n\\n</td>\n",
       "      <td>video</td>\n",
       "      <td>NaN</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c_0000c03adc8d</td>\n",
       "      <td>Nado de aproximação</td>\n",
       "      <td>Neste vídeo você vai aprender o nado de aproxi...</td>\n",
       "      <td>document</td>\n",
       "      <td>\\nNado de aproximação\\nSaber nadar nas ondas ...</td>\n",
       "      <td>pt</td>\n",
       "      <td>Sikana Education</td>\n",
       "      <td>CC BY-NC-ND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c_00016694ea2a</td>\n",
       "      <td>geometry-m3-topic-a-overview.pdf</td>\n",
       "      <td>geometry-m3-topic-a-overview.pdf</td>\n",
       "      <td>document</td>\n",
       "      <td>Estándares Comunes del Estado de Nueva York\\n\\...</td>\n",
       "      <td>es</td>\n",
       "      <td>Engage NY</td>\n",
       "      <td>CC BY-NC-SA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                             title  \\\n",
       "0  c_00002381196d  Sumar números de varios dígitos: 48,029+233,930    \n",
       "1  c_000087304a9e                    Trovare i fattori di un numero   \n",
       "2  c_0000ad142ddb                           Sumar curvas de demanda   \n",
       "3  c_0000c03adc8d                               Nado de aproximação   \n",
       "4  c_00016694ea2a                  geometry-m3-topic-a-overview.pdf   \n",
       "\n",
       "                                         description      kind  \\\n",
       "0  Suma 48,029+233,930 mediante el algoritmo está...     video   \n",
       "1                    Sal trova i fattori di 120.\\n\\n     video   \n",
       "2                  Cómo añadir curvas de demanda\\n\\n     video   \n",
       "3  Neste vídeo você vai aprender o nado de aproxi...  document   \n",
       "4                   geometry-m3-topic-a-overview.pdf  document   \n",
       "\n",
       "                                                text language  \\\n",
       "0                                                NaN       es   \n",
       "1                                                NaN       it   \n",
       "2                                                NaN       es   \n",
       "3  \\nNado de aproximação\\nSaber nadar nas ondas ...       pt   \n",
       "4  Estándares Comunes del Estado de Nueva York\\n\\...       es   \n",
       "\n",
       "   copyright_holder      license  \n",
       "0               NaN          NaN  \n",
       "1               NaN          NaN  \n",
       "2               NaN          NaN  \n",
       "3  Sikana Education  CC BY-NC-ND  \n",
       "4         Engage NY  CC BY-NC-SA  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Content\n",
    "content = pd.read_csv(\"content.csv\")\n",
    "print(np.shape(content))\n",
    "content.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615 60902\n"
     ]
    }
   ],
   "source": [
    "#Split train/test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "corr_train, corr_test = train_test_split(\n",
    "    correlations,\n",
    "    train_size = 0.01,\n",
    "    test_size = 0.99,\n",
    "    random_state = 10,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "print(\n",
    "    len(corr_train),\n",
    "    len(corr_test)\n",
    ")\n",
    "\n",
    "#We train a very small subset of data because Siamese Neural Networks compare each entry to another, leading to n^2 comparisons.\n",
    "#So, 615 training observations will lead to 615^2 (378,225) comparisons being made.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(correlations, topics, content):\n",
    "    #Drop/combine columns\n",
    "    content[\"text\"] = content[\"text\"].fillna('')\n",
    "    content = content.dropna()\n",
    "    content_combined = content[\"language\"] + \" \" + content[\"title\"] + \" \" + content[\"description\"] + \" \" + content[\"text\"]\n",
    "    content_combined = pd.DataFrame({\"id\":content[\"id\"], \"features\":content_combined})\n",
    "\n",
    "    topics[\"description\"] = topics[\"description\"].fillna('')\n",
    "    topics = topics.dropna()\n",
    "    topics_combined = topics[\"language\"] + \" \" + topics[\"title\"] + \" \" + topics[\"description\"] + \" \" + topics[\"channel\"]\n",
    "    topics_combined = pd.DataFrame({\"id\":topics[\"id\"], \"features\":topics_combined})\n",
    "\n",
    "    #Explode correlations rows\n",
    "    correlations[\"content_ids\"] = correlations[\"content_ids\"].str.split()\n",
    "    correlations = correlations.explode(\"content_ids\")\n",
    "\n",
    "    #Merge\n",
    "    merged = correlations.merge(topics_combined, how=\"inner\", left_on=\"topic_id\", right_on=\"id\")\n",
    "    merged = merged.reset_index().merge(content_combined, how=\"inner\", left_on=\"content_ids\", right_on=\"id\", sort=False, suffixes=(\"_topics\", \"_content\")).sort_values(axis=0, by=\"index\")\n",
    "    merged = merged.drop([\"content_ids\", \"topic_id\"], axis=1)\n",
    "\n",
    "    #Split\n",
    "    corr_topics = merged[['index', 'features_topics']]\n",
    "    corr_topics.columns = ['index', 'features']\n",
    "    corr_content = merged[['index', 'features_content']]\n",
    "    corr_content.columns = ['index', 'features']\n",
    "\n",
    "    corr_topic_ids = merged[['id_topics']]\n",
    "    corr_content_ids = merged[['id_content']]\n",
    "\n",
    "    return corr_topics, corr_content, corr_topic_ids, corr_content_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index                                           features\n",
      "0     13  es Ondas En este recurso nos preguntamos: ¿Por...\n",
      "2     14  es Las radiaciones en la vida cotidiana En est...\n",
      "5     15  pt Curiosidades Matemáticas: O porquê da regra...\n",
      "7     17  pt Para Saber Mais!: Reconhecimento de Números...\n",
      "9     21  es Cómo escribir expresiones con variables y p...\n",
      "    index                                           features\n",
      "0       5  pt Entradas e saídas de uma função Entenda um ...\n",
      "3       6  pt Entradas e saídas de uma função Entenda um ...\n",
      "6       8                             en Transcripts  6e3ba4\n",
      "7       9  bg Графики на експоненциални функции (Алгебра ...\n",
      "10     10  bg Графики на експоненциални функции (Алгебра ...\n"
     ]
    }
   ],
   "source": [
    "#Test case\n",
    "from copy import deepcopy\n",
    "topics_copy = deepcopy(topics)\n",
    "content_copy = deepcopy(content)\n",
    "topics_copy2 = deepcopy(topics)\n",
    "content_copy2 = deepcopy(content)\n",
    "topics_copy3 = deepcopy(topics)\n",
    "content_copy3 = deepcopy(content)\n",
    "corr_copy = deepcopy(correlations)\n",
    "\n",
    "train_topics, train_content, train_topic_ids, train_content_ids = combine(corr_train, topics_copy, content_copy)\n",
    "train_topics.head()\n",
    "train_content.head()\n",
    "\n",
    "test_topics, test_content, test_topic_ids, test_content_ids= combine(corr_test, topics_copy2, content_copy2)\n",
    "test_topics.head()\n",
    "print(test_content.head())\n",
    "\n",
    "all_topics, all_content, all_topics_ids, all_content_ids = combine(corr_copy, topics_copy3, content_copy3)\n",
    "print(all_topics.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id_topics\n",
      "0   t_00068291e9a4\n",
      "3   t_00068291e9a4\n",
      "6   t_00069b63a70a\n",
      "7   t_0006d41a73a8\n",
      "10  t_0006d41a73a8\n"
     ]
    }
   ],
   "source": [
    "print(all_topics_ids.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create TF Data Pipeline\n",
    "import tensorflow as tf\n",
    "\n",
    "#Train\n",
    "train_topics_raw = tf.data.Dataset.from_tensor_slices(\n",
    "    tf.cast(train_topics.features, tf.string)\n",
    ")\n",
    "\n",
    "train_content_raw = tf.data.Dataset.from_tensor_slices(\n",
    "    tf.cast(train_content.features, tf.string)\n",
    ")\n",
    "\n",
    "#Test\n",
    "test_topics_raw = tf.data.Dataset.from_tensor_slices(\n",
    "    tf.cast(test_topics.features, tf.string)\n",
    ")\n",
    "\n",
    "test_content_raw = tf.data.Dataset.from_tensor_slices(\n",
    "    tf.cast(test_content.features, tf.string)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version: 2.9.1\n",
      "keras version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "print(\"tf version:\",tf.__version__)\n",
    "\n",
    "print(\"keras version:\", tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "VOCAB_SIZE = 100000\n",
    "MAX_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Polar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Custom standardization function\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#lang_code_dict is a dictionary of all languages present in the dataset\n",
    "lang_dict = {\n",
    "    \"en\":\"english\",\n",
    "    \"es\":\"spanish\",\n",
    "    \"it\":\"italian\",\n",
    "    'pt':\"portuguese\",\n",
    "    'mr':'marathi',\n",
    "    'bg':'bulgarian',\n",
    "    'gu':'gujarati',\n",
    "    'sw':'swahili',\n",
    "    'hi':'hindi',\n",
    "    'ar':'arabic',\n",
    "    'bn':'bengali',\n",
    "    'as':'assamese',\n",
    "    'zh':'chinese',\n",
    "    'fr':'french',\n",
    "    'km':'khmer',\n",
    "    'pl':'polish',\n",
    "    'ta':'tamil',\n",
    "    'or':'oriya',\n",
    "    'ru':'russian',\n",
    "    'kn':'kannada',\n",
    "    'swa':'swahili',\n",
    "    'my':'burmese',\n",
    "    'pnb':'punjabi',\n",
    "    'fil':'filipino',\n",
    "    'tr':'turkish',\n",
    "    'te':'telugu',\n",
    "    'ur':'urdu'\n",
    "}\n",
    "\n",
    "#supported_languages is a list of languages supported by the natural language tool kit (NLTK) module. \n",
    "supported_languages = stopwords.fileids()\n",
    "\n",
    "def custom_standardize(input_string):\n",
    "    #basic cleaning\n",
    "    lower = tf.strings.lower(input_string, encoding='utf-8')\n",
    "    no_stars = tf.strings.regex_replace(lower, \"\\*\", \" \")\n",
    "    no_newline = tf.strings.regex_replace(no_stars, \"\\n\", \"\")\n",
    "    no_digits = tf.strings.regex_replace(no_newline, \"\\w*\\d\\w*\",\"\")\n",
    "\n",
    "    #testing\n",
    "    lang_code = input_string[0:2]\n",
    "    if lang_code == 'en':\n",
    "        for word in stopwords.words('english'):\n",
    "            no_stopwords = tf.strings.regex_replace(no_digits, ' ' + word + ' ', \" \")\n",
    "\n",
    "    \n",
    "    \n",
    "    no_punctuations = tf.strings.regex_replace(no_digits, f\"([{string.punctuation}])\", r\" \")\n",
    "\n",
    "    #testing\n",
    "    \n",
    "    #remove stopwords\n",
    "    # lang_code = input_string[0:2]\n",
    "    # no_stopwords = ' ' + no_punctuations + ' '\n",
    "    # if lang_code_dict[lang_code] in supported_languages:\n",
    "    #     for word in stopwords.words(lang_code_dict[lang_code]):\n",
    "    #         no_stopwords = tf.strings.regex_replace(no_punctuations, ' ' + word[0] + ' ', r\" \")\n",
    "    #     no_extra_space = tf.strings.regex_replace(no_stopwords, \" +\",\" \")\n",
    "\n",
    "    \n",
    "    out = tf.strings.strip(no_stopwords)\n",
    "    #out = tf.strings.strip(no_punctuations)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_standardize(input_string):\n",
    "    input_string = tf.strings.lower(input_string, encoding='utf-8') #lowercase\n",
    "    input_string = tf.strings.regex_replace(input_string, f\"([{string.punctuation}])\", r\"\") #remove punctuation\n",
    "    input_string = tf.strings.regex_replace(input_string, '\\n', \"\") #remove newlines\n",
    "    input_string = tf.strings.regex_replace(input_string, '\\w*\\d\\w*', \"\") #remove digits\n",
    "    input_string = tf.strings.regex_replace(input_string, ' +', \" \") #remove 2+ whitespaces\n",
    "    input_string = tf.strings.strip(input_string) #remove leading and tailing whitespaces\n",
    "\n",
    "    return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input string:  tf.Tensor(b\"en hello,, my' ^name is taro1234 and 1 1ike to build models!.,  \", shape=(), dtype=string)\n",
      "output string:  en hello name build models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test case\n",
    "input_string = tf.strings.lower(\"en HELLO,, my' ^nAme is Taro1234 and 1 1ike to build models!.,  \", encoding='utf-8')\n",
    "print(\"input string: \", input_string)\n",
    "\n",
    "output_string = test_standardize(input_string)\n",
    "print(\"output string: \", output_string.numpy().decode('utf-8'))\n",
    "#print(\"output string: \", output_string)\n",
    "\n",
    "lang_dict['en'] in supported_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text vectorization layer\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize = \"lower_and_strip_punctuation\",\n",
    "    #standardize = test_standardize,\n",
    "    split = \"whitespace\",\n",
    "    max_tokens = VOCAB_SIZE + 2,\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = MAX_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapt text vectorization layer\n",
    "vectorize_layer.adapt(train_content.features)\n",
    "#vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert text to tf.string \n",
    "def convert_text_input(sample):\n",
    "    text = sample\n",
    "    text = tf.expand_dims(text, -1)  \n",
    "    return tf.squeeze(vectorize_layer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_content_mapped = train_content_raw.map(convert_text_input, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "train_topics_mapped = train_topics_raw.map(convert_text_input, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "test_topics_mapped = test_topics_raw.map(convert_text_input, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "test_content_mapped = test_content_raw.map(convert_text_input, num_parallel_calls=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 3254  9418     1 51922     1     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0], shape=(50,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[ 3254  9418     1 51922     1     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0], shape=(50,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[ 3254  9418     1 51922     1     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0], shape=(50,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for each in train_topics_mapped.take(3):\n",
    "  print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.zip(\n",
    "    (train_topics_mapped, train_content_raw)\n",
    ")\n",
    "\n",
    "test_ds = tf.data.Dataset.zip(\n",
    "    (test_topics_mapped, test_content_raw)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_ds.take(1):\n",
    "  print(\"input (topic) x.shape: \", x.shape)\n",
    "  print(\"output (content) y.shape: \", y.shape)\n",
    "  print(\"input (topic) x: \", x)\n",
    "  print(\"output (content) y: \",y)\n",
    "#   input = \" \".join([vocab[_] for _ in np.squeeze(x)])\n",
    "#   output = id_to_category[y.numpy()]\n",
    "#   print(\"x: input (review) in text: \" , input)\n",
    "#   print(\"y: output (category) in text: \" , output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finalize data pipeline\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = train_ds.cardinality().numpy()\n",
    "\n",
    "train_ds = train_ds.shuffle(buffer_size=BUFFER_SIZE)\\\n",
    "    .batch(batch_size = BATCH_SIZE, drop_remainder=True)\\\n",
    "        .cache()\\\n",
    "            .prefetch(AUTOTUNE)\n",
    "\n",
    "test_ds = test_ds.shuffle(buffer_size=BUFFER_SIZE)\\\n",
    "    .batch(batch_size = BATCH_SIZE, drop_remainder=True)\\\n",
    "        .cache()\\\n",
    "            .prefetch(AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=<unknown>, dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(64,), dtype=tf.string, name=None))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " inp_topics (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " inp_content (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " sequential_2 (Sequential)      (None, 64)           26419264    ['inp_content[0][0]',            \n",
      "                                                                  'inp_topics[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 128)          0           ['sequential_2[1][0]',           \n",
      "                                                                  'sequential_2[0][0]']           \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64)           8256        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1)            65          ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 26,427,585\n",
      "Trainable params: 26,427,585\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "inp_content = Input((1, ), name='inp_content') \n",
    "inp_topics = Input((1, ), name='inp_topics')\n",
    "\n",
    "snn = Sequential([\n",
    "  Reshape((1, )),\n",
    "  vectorize_layer,\n",
    "  Embedding(VOCAB_SIZE, 256),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Flatten(),\n",
    "  Dense(64, activation='relu'),\n",
    "])\n",
    "\n",
    "feature_vector_content = snn(inp_content)\n",
    "feature_vector_topics = snn(inp_topics)\n",
    "\n",
    "concat = Concatenate()([feature_vector_topics, feature_vector_content])\n",
    "\n",
    "dense = Dense(64, activation='relu')(concat)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "model = Model(inputs=[inp_topics, inp_content], outputs=output)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Making Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def pair_data(topics, content):\n",
    "    text_pairs, id_pairs = [], []\n",
    "    tuples = [(x1, y1) for x1, y1 in zip(topics, content)]\n",
    "\n",
    "    for t in itertools.product(tuples, tuples):\n",
    "        item_A, item_B = t\n",
    "        index_A, text_A = t[0]\n",
    "        index_B, text_B = t[1]\n",
    "    \n",
    "    is_match = int(index_A == index_B)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4703bde01830f6d095c98ee537b927e3a42233672027169c1dba6e7efc366de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
