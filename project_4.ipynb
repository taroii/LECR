{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Equality - Curriculum Recommendations Project\n",
    "\n",
    "Taro Iyadomi (UCLA Data Theory '24)\n",
    "\n",
    "12/23/2022 - Present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary packages/functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "import random\n",
    "import itertools\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample Submission\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "#Correlations\n",
    "correlations = pd.read_csv(\"correlations.csv\")\n",
    "\n",
    "#Topics\n",
    "topics = pd.read_csv(\"topics.csv\")\n",
    "\n",
    "#Content\n",
    "content = pd.read_csv(\"content.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Preparing Data For Training/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(correlations, topics, content):\n",
    "    #Drop/combine columns\n",
    "    content[\"text\"] = content[\"text\"].fillna('')\n",
    "    content = content.dropna()\n",
    "    content_combined = content[\"language\"] + \" \" + content[\"title\"] + \" \" + content[\"description\"] + \" \" + content[\"text\"]\n",
    "    content_combined = pd.DataFrame({\"id\":content[\"id\"], \"features\":content_combined})\n",
    "\n",
    "    topics[\"description\"] = topics[\"description\"].fillna('')\n",
    "    topics = topics.dropna()\n",
    "    topics_combined = topics[\"language\"] + \" \" + topics[\"channel\"] + ' ' + topics[\"title\"] + \" \" + topics[\"description\"]\n",
    "    topics_combined = pd.DataFrame({\"id\":topics[\"id\"], \"features\":topics_combined})\n",
    "\n",
    "    #Merge\n",
    "    merged = correlations.merge(topics_combined, how=\"inner\", left_on=\"topic_id\", right_on=\"id\")\n",
    "    merged = merged.reset_index().merge(content_combined, how=\"left\", left_on=\"content_ids\", right_on=\"id\", sort=False, suffixes=(\"_topics\", \"_content\")).sort_values(axis=0, by=\"index\")\n",
    "    merged = merged.drop([\"content_ids\", \"topic_id\"], axis=1)\n",
    "\n",
    "    #Split\n",
    "    corr_topics = merged[['index', 'features_topics']]\n",
    "    corr_topics.columns = ['id', 'features']\n",
    "    corr_content = merged[['index', 'features_content']]\n",
    "    corr_content.columns = ['id', 'features']\n",
    "\n",
    "    index_to_topic = pd.Series(merged.id_topics.values, index=merged.index).to_dict()\n",
    "    index_to_content = pd.Series(merged.id_content.values, index=merged.index).to_dict()\n",
    "\n",
    "    return corr_topics, corr_content, index_to_topic, index_to_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_topics, corr_content, index_to_topic, index_to_content = combine(correlations, topics, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a stopword removal function to remove stopwords for each language\n",
    "#dictionary of languages found in our data\n",
    "lang_dict = {\n",
    "    \"en\":\"english\",\n",
    "    \"es\":\"spanish\",\n",
    "    \"it\":\"italian\",\n",
    "    'pt':\"portuguese\",\n",
    "    'mr':'marathi',\n",
    "    'bg':'bulgarian',\n",
    "    'gu':'gujarati',\n",
    "    'sw':'swahili',\n",
    "    'hi':'hindi',\n",
    "    'ar':'arabic',\n",
    "    'bn':'bengali',\n",
    "    'as':'assamese',\n",
    "    'zh':'chinese',\n",
    "    'fr':'french',\n",
    "    'km':'khmer',\n",
    "    'pl':'polish',\n",
    "    'ta':'tamil',\n",
    "    'or':'oriya',\n",
    "    'ru':'russian',\n",
    "    'kn':'kannada',\n",
    "    'swa':'swahili',\n",
    "    'my':'burmese',\n",
    "    'pnb':'punjabi',\n",
    "    'fil':'filipino',\n",
    "    'tr':'turkish',\n",
    "    'te':'telugu',\n",
    "    'ur':'urdu',\n",
    "    'fi':'finnish',\n",
    "    'pn':'unknown',\n",
    "    'mu':'unknown'}\n",
    "\n",
    "#list of languages supported by the natural language tool kit (NLTK) module.\n",
    "supported_languages = stopwords.fileids()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    lang_code = text[0:2]\n",
    "    if lang_dict[lang_code] in supported_languages:\n",
    "        for word in stopwords.words(lang_dict[lang_code]):\n",
    "            text = text.replace(' ' + word + ' ', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_topics[\"features\"] = corr_topics.features.apply(remove_stopwords)\n",
    "corr_content[\"features\"] = corr_content.features.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10) #For repeatablity\n",
    "train_indices = random.sample(range(len(corr_content)), round(0.8*len(corr_content))) #80/20 train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Split training data so 50% is matching and 50% is not matching\n",
    "half = round(len(train_indices) / 2)\n",
    "full = len(train_indices)\n",
    "\n",
    "train_topics_half = corr_topics.iloc[train_indices[:half], :]\n",
    "train_content_half = corr_content.iloc[train_indices[:half], :]\n",
    "\n",
    "#Shift second half so that topics/content are not matching\n",
    "train_topics_full = corr_topics.iloc[train_indices[half:(full-20)], :] \n",
    "train_content_full = corr_content.iloc[train_indices[(half+20):(full)], :] \n",
    "\n",
    "train_topics = pd.concat([train_topics_half, train_topics_full]).reset_index().drop(\"index\", axis=1)\n",
    "train_content = pd.concat([train_content_half, train_content_full]).reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Repeat for test data\n",
    "test_topics = corr_topics.drop(train_indices, axis=0)\n",
    "test_content = corr_content.drop(train_indices, axis=0)\n",
    "\n",
    "half = round(len(test_topics.features) / 2)\n",
    "full = len(test_topics.features)\n",
    "\n",
    "test_topics_half = test_topics.iloc[:half, :]\n",
    "test_content_half = test_content.iloc[:half, :]\n",
    "\n",
    "test_topics_full = test_topics.iloc[half:(full - 5), :]\n",
    "test_content_full = test_content.iloc[(half+5):full, :]\n",
    "\n",
    "test_topics = pd.concat([test_topics_half, test_topics_full]).reset_index().drop(\"index\", axis=1)\n",
    "test_content = pd.concat([test_content_half, test_content_full]).reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array((train_topics.id == train_content.id).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.array((test_topics.id == test_content.id).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Convert data to tensors\n",
    "train_topics = tf.data.Dataset.from_tensor_slices(tf.cast(train_topics.features, tf.string))\n",
    "train_content = tf.data.Dataset.from_tensor_slices(tf.cast(train_content.features, tf.string))\n",
    "train_labels = tf.data.Dataset.from_tensor_slices(tf.cast(train_labels, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_topics = tf.data.Dataset.from_tensor_slices(tf.cast(test_topics.features, tf.string))\n",
    "test_content = tf.data.Dataset.from_tensor_slices(tf.cast(test_content.features, tf.string))\n",
    "test_labels = tf.data.Dataset.from_tensor_slices(tf.cast(test_labels, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Combine data into TensorFlow Datasets\n",
    "#### Perfectly shuffle, batch, cache, and prefetch our new datasets\n",
    "train_ds = tf.data.Dataset.zip(\n",
    "    ((train_topics, train_content), train_labels)\n",
    ")\n",
    "\n",
    "train_ds = train_ds.shuffle(buffer_size = train_ds.cardinality().numpy()).batch(batch_size = 64).cache().prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.data.Dataset.zip(\n",
    "    ((test_topics, test_content), test_labels)\n",
    ")\n",
    "\n",
    "test_ds = test_ds.shuffle(buffer_size = test_ds.cardinality().numpy()).batch(batch_size = 64).cache().prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "VOCAB_SIZE = 1000000\n",
    "MAX_LEN = 50\n",
    "\n",
    "#Custom standardization function\n",
    "def my_standardize(text):    \n",
    "    text = tf.strings.lower(text, encoding='utf-8') #lowercase\n",
    "    text = tf.strings.regex_replace(text, f\"([{string.punctuation}])\", r\" \") #remove punctuation\n",
    "    text = tf.strings.regex_replace(text, '\\n', \"\") #remove newlines\n",
    "    text = tf.strings.regex_replace(text, ' +', \" \") #remove 2+ whitespaces\n",
    "    text = tf.strings.strip(text) #remove leading and tailing whitespaces\n",
    "    return text\n",
    "\n",
    "#Text vectorization layer\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize = my_standardize,\n",
    "    split = \"whitespace\",\n",
    "    max_tokens = VOCAB_SIZE + 2,\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = MAX_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(pd.concat([corr_topics[\"features\"], corr_content[\"features\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save layer to hard drive\n",
    "# vectorize_layer_model = tf.keras.models.Sequential()\n",
    "# vectorize_layer_model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "# vectorize_layer_model.add(vectorize_layer)\n",
    "# vectorize_layer_model.save(\"vectorize_layer\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_topics = Input((1, ), dtype=tf.string)\n",
    "inp_content = Input((1, ), dtype=tf.string)\n",
    "\n",
    "vectorized_topics = vectorize_layer(inp_topics)\n",
    "vectorized_content = vectorize_layer(inp_content)\n",
    "\n",
    "snn = Sequential([ \n",
    "  Embedding(VOCAB_SIZE, 256),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Flatten(),\n",
    "  Dense(128, activation='relu'),\n",
    "])\n",
    "\n",
    "snn_content = snn(vectorized_content)\n",
    "snn_topics = snn(vectorized_topics)\n",
    "\n",
    "concat = Concatenate()([snn_topics, snn_content])\n",
    "\n",
    "dense = Dense(64, activation='relu')(concat)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "model = Model(inputs=[inp_topics, inp_content], outputs=output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=tf.keras.metrics.BinaryAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runtime: 142 min 9.7 sec\n",
    "train_ds.cardinality().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"model\", save_format=\"tf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds, verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Making Predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must now make predictions on the topics dataset. While the topics dataset is very large, the correlations data makes up for a large portion of it. \n",
    "So, we are left with only about 15,000 topics to cover!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4703bde01830f6d095c98ee537b927e3a42233672027169c1dba6e7efc366de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
